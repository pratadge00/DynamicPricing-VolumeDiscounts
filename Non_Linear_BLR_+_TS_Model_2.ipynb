{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b07d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 0 · Imports & global configuration\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility ─────────────────────────────────────────────\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Dataset location ────────────────────────────────────────────\n",
    "FILE_PATH = r'D:\\IITR DS Final Year Thesis\\Dataset Superstore\\archive\\generated_single_product_dataset_with_seasonal_variation.csv'\n",
    "\n",
    "# Economic constants (update later if you have product‑specific info)\n",
    "COST_PER_UNIT_DEFAULT = 0.0        # will be overwritten by df['cost'].mean()\n",
    "PRICE_GRID_SIZE       = 200        # resolution when searching for optimal price\n",
    "TANH_SCALE            = 150        #   p / TANH_SCALE inside tanh(·)\n",
    "\n",
    "print(\"✔️  Block 0 ready – libraries imported, seed fixed.\")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 1 · Data loading & feature engineering\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# • Reads the CSV at FILE_PATH\n",
    "# • Parses & sorts dates\n",
    "# • Adds weekly dummies, annual sin/cos\n",
    "# • Builds non‑linear price bases: price_sq, log_price, tanh_price\n",
    "# • Prepares feature‑name lists for later blocks\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Load -----------------------------------------------------------------\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# 2) Basic sanity checks --------------------------------------------------\n",
    "assert df['price_paid'].notna().all(), \"Missing price data!\"\n",
    "df.rename(columns={'price_paid': 'price'}, inplace=True)   # convenience alias\n",
    "\n",
    "# 3) Seasonality indices --------------------------------------------------\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['day_of_week'] = df['date'].dt.weekday         # Monday=0 … Sunday=6\n",
    "\n",
    "# 3a) Weekly dummies (baseline = Sunday, i.e. day 6)\n",
    "weekly_dummies = pd.get_dummies(df['day_of_week'], prefix='dow', drop_first=True)\n",
    "df = pd.concat([df, weekly_dummies], axis=1)\n",
    "weekly_cols = weekly_dummies.columns.tolist()\n",
    "\n",
    "# 3b) Annual Fourier terms\n",
    "df['sin_annual'] = np.sin(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "df['cos_annual'] = np.cos(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "\n",
    "# 4) Price‑basis functions -----------------------------------------------\n",
    "df['price_sq']   = df['price'] ** 2\n",
    "df['log_price']  = np.log(df['price'])\n",
    "df['tanh_price'] = np.tanh(df['price'] / TANH_SCALE)\n",
    "\n",
    "price_basis_cols = ['price', 'price_sq', 'log_price', 'tanh_price']\n",
    "\n",
    "# 5) Feature‑name lists ---------------------------------------------------\n",
    "static_feature_cols = weekly_cols + ['sin_annual', 'cos_annual']\n",
    "feature_cols        = ['intercept'] + static_feature_cols + price_basis_cols\n",
    "\n",
    "# 6) Derived globals ------------------------------------------------------\n",
    "MIN_PRICE, MAX_PRICE = df['price'].min(), df['price'].max()\n",
    "COST_PER_UNIT        = df['cost'].mean() if 'cost' in df.columns else COST_PER_UNIT_DEFAULT\n",
    "\n",
    "# 7) Quick printout -------------------------------------------------------\n",
    "print(\"DataFrame shape :\", df.shape)\n",
    "print(\"Feature columns  :\", feature_cols)\n",
    "print(f\"Price range     : {MIN_PRICE:.2f} – {MAX_PRICE:.2f}\")\n",
    "print(f\"Avg. unit cost  : {COST_PER_UNIT:.2f}\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"✔️  Block 1 complete – dataset ready & features engineered.\")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 2 · Prior specification & initial posterior\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Assumes Block 0 and Block 1 have run and defined:\n",
    "#   • df, feature_cols, static_feature_cols, price_basis_cols\n",
    "#   • WEEKLY_COLS etc.\n",
    "# Produces:\n",
    "#   • mu0, Sigma0           – prior mean / covariance\n",
    "#   • sigma2                – obs‑noise variance\n",
    "#   • post_mean, post_cov   – initial posterior (equal to prior)\n",
    "#   • feat_idx              – dict: feature name ➜ index (handy later)\n",
    "#   • price_term_indices    – list of indices for price‑basis coeffs\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Feature index helper ------------------------------------\n",
    "feat_idx = {name: i for i, name in enumerate(feature_cols)}\n",
    "\n",
    "# For convenience: indices of price‑related terms\n",
    "price_term_indices = [feat_idx[col] for col in price_basis_cols]\n",
    "\n",
    "# 2) Prior mean vector (μ₀) -----------------------------------\n",
    "mu0 = np.zeros(len(feature_cols))\n",
    "\n",
    "# Intercept prior: start near median quantity\n",
    "mu0[feat_idx['intercept']] = df['quantity'].median()\n",
    "\n",
    "# Monotonic priors on price terms (negative to enforce ↓ demand)\n",
    "mu0[feat_idx['price']]       = -1.0      # linear price slope\n",
    "mu0[feat_idx['price_sq']]    = -0.05     # gentle concavity\n",
    "mu0[feat_idx['log_price']]   = -1.0\n",
    "mu0[feat_idx['tanh_price']]  = -1.0\n",
    "\n",
    "# Seasonal terms → mean 0 (no initial bias)\n",
    "\n",
    "# 3) Prior standard deviations (σ₀) ---------------------------\n",
    "prior_sd = np.ones(len(feature_cols)) * 10.0          # default weak\n",
    "prior_sd[feat_idx['intercept']] = 20.0                # looser on level\n",
    "# Weekly dummies & annual sin/cos: moderate prior\n",
    "for col in static_feature_cols:\n",
    "    prior_sd[feat_idx[col]] = 5.0\n",
    "# Price terms: a bit tighter to keep monotonicity strong\n",
    "for idx in price_term_indices:\n",
    "    prior_sd[idx] = 2.0\n",
    "\n",
    "# 4) Prior covariance (Σ₀) ------------------------------------\n",
    "Sigma0 = np.diag(prior_sd ** 2)        # diagonal (independent priors)\n",
    "\n",
    "# 5) Observation noise variance (σ²) --------------------------\n",
    "sigma_hat = df['quantity'].std()\n",
    "sigma2    = sigma_hat ** 2\n",
    "\n",
    "# 6) Initialise posterior = prior -----------------------------\n",
    "post_mean = mu0.copy()\n",
    "post_cov  = Sigma0.copy()\n",
    "\n",
    "# 7) Sanity summary -------------------------------------------\n",
    "import pandas as pd\n",
    "print(\"\\n──────── Prior mean (μ₀) ────────\")\n",
    "print(pd.Series(mu0, index=feature_cols).round(3))\n",
    "print(\"\\n──────── Prior SD (σ₀) ──────────\")\n",
    "print(pd.Series(prior_sd, index=feature_cols).round(3))\n",
    "print(f\"\\nObservation noise σ²  ≈ {sigma2:.2f}\")\n",
    "print(f\"Feature count         = {len(feature_cols)}\")\n",
    "print(\"✔️  Block 2 ready – posterior initialised.\")\n",
    "\n",
    "# 8) Export key objects for later blocks ----------------------\n",
    "# (They remain in memory; nothing to return explicitly.)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 3 · Utility helpers\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# These pure‑Python helpers keep the main loop (Block 4) tidy.\n",
    "#\n",
    "# Assumes Blocks 0–2 defined globals:\n",
    "#   • feature_cols, static_feature_cols, price_basis_cols\n",
    "#   • weekly_cols, MIN_PRICE, MAX_PRICE, PRICE_GRID_SIZE\n",
    "#   • TANH_SCALE, COST_PER_UNIT, sigma2\n",
    "#\n",
    "# Exposes:\n",
    "#   build_static_vector(row)        – fixed context part (no price)\n",
    "#   price_basis(price)              – non‑linear price features\n",
    "#   make_feature_vector(row, p)     – full φ vector for any price\n",
    "#   blr_update(mu, Sigma, x, y)     – Kalman update (BLR)\n",
    "#   sample_theta(mu, Sigma)         – θ ~ N(μ, Σ)\n",
    "#   demand_pred(theta, static, p)   – deterministic demand (no noise)\n",
    "#   find_optimal_price(theta, static) – argmax_p (p−c)·vθ(p)\n",
    "#   clamp(val, lo, hi)              – scalar clamp\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 1)  Context (static) vector builder ------------------------\n",
    "def build_static_vector(row):\n",
    "    \"\"\"\n",
    "    Return the intercept + weekly dummies + annual sin/cos\n",
    "    for *this transaction’s date* (price terms excluded).\n",
    "    Result length = 1 + len(static_feature_cols) − (dropping price bases).\n",
    "    \"\"\"\n",
    "    vec = [1.0]                                       # intercept\n",
    "    # weekly dummies in the same order as weekly_cols\n",
    "    vec.extend([row[col] for col in weekly_cols])\n",
    "    # annual fourier\n",
    "    vec.append(row['sin_annual'])\n",
    "    vec.append(row['cos_annual'])\n",
    "    return np.asarray(vec, dtype=float)               # shape (len(static)+1,)\n",
    "\n",
    "# 2)  Price‑basis builder ------------------------------------\n",
    "def price_basis(price):\n",
    "    \"\"\"\n",
    "    Compute non‑linear transformations of price.\n",
    "    Order must match price_basis_cols.\n",
    "    \"\"\"\n",
    "    return np.asarray([\n",
    "        price,\n",
    "        price**2,\n",
    "        math.log(price),\n",
    "        math.tanh(price / TANH_SCALE)\n",
    "    ], dtype=float)\n",
    "\n",
    "# 3)  Full feature vector ------------------------------------\n",
    "def make_feature_vector(row, price):\n",
    "    \"\"\"\n",
    "    Concatenate static context vector and price bases.\n",
    "    Length = len(feature_cols)\n",
    "    \"\"\"\n",
    "    static_vec = build_static_vector(row)\n",
    "    return np.concatenate([static_vec, price_basis(price)])\n",
    "\n",
    "# 4)  Kalman / BLR one‑step update ---------------------------\n",
    "def blr_update(mu, Sigma, x_vec, y_obs, sigma2=sigma2):\n",
    "    \"\"\"\n",
    "    Posterior update for Bayesian linear regression with\n",
    "    known Gaussian noise variance.\n",
    "    Args\n",
    "      mu     – prior mean        (d,)\n",
    "      Sigma  – prior covariance  (d,d)\n",
    "      x_vec  – feature vector    (d,)\n",
    "      y_obs  – observed target   (scalar)\n",
    "    Returns\n",
    "      new_mu, new_Sigma\n",
    "    \"\"\"\n",
    "    x = x_vec.reshape(-1, 1)                        # (d,1)\n",
    "    s = float(x.T @ Sigma @ x + sigma2)             # predictive variance\n",
    "    K = (Sigma @ x) / s                             # Kalman gain (d,1)\n",
    "    residual = y_obs - float(mu @ x_vec)            # innovation\n",
    "    new_mu = mu + K.flatten() * residual\n",
    "    new_Sigma = Sigma - K @ (x.T @ Sigma)\n",
    "    # Symmetrise to avoid numeric drift\n",
    "    new_Sigma = 0.5 * (new_Sigma + new_Sigma.T)\n",
    "    return new_mu, new_Sigma\n",
    "\n",
    "# 5)  Posterior sample ---------------------------------------\n",
    "def sample_theta(mu, Sigma):\n",
    "    \"\"\"Draw θ ~ N(μ, Σ).\"\"\"\n",
    "    return np.random.multivariate_normal(mu, Sigma)\n",
    "\n",
    "# 6)  Deterministic demand prediction ------------------------\n",
    "def demand_pred(theta, static_vec, price):\n",
    "    \"\"\"vθ(p) without observation noise (can be negative early).\"\"\"\n",
    "    return float(theta[:-len(price_basis_cols)] @ static_vec + theta[-len(price_basis_cols):] @ price_basis(price))\n",
    "\n",
    "# 7)  Optimal price for a sampled θ --------------------------\n",
    "def find_optimal_price(theta, static_vec,\n",
    "                       price_bounds=(MIN_PRICE, MAX_PRICE),\n",
    "                       grid_size=PRICE_GRID_SIZE,\n",
    "                       cost=COST_PER_UNIT):\n",
    "    \"\"\"\n",
    "    Grid‑search argmax_p (p−c)·vθ(p) within [min,max].\n",
    "    Returns the best price p* for this θ and context.\n",
    "    \"\"\"\n",
    "    prices = np.linspace(price_bounds[0], price_bounds[1], grid_size)\n",
    "    best_p, best_val = prices[0], -np.inf\n",
    "    for p in prices:\n",
    "        q = demand_pred(theta, static_vec, p)\n",
    "        if q < 0:           # enforce non‑negative demand estimate\n",
    "            q = 0\n",
    "        profit = (p - cost) * q\n",
    "        if profit > best_val:\n",
    "            best_val, best_p = profit, p\n",
    "    return best_p\n",
    "\n",
    "# 8)  Scalar clamp convenience -------------------------------\n",
    "def clamp(val, lo, hi):\n",
    "    return max(lo, min(hi, val))\n",
    "\n",
    "print(\"✔️  Block 3 loaded – helper functions ready.\")\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 4 · Online Thompson‑Sampling loop\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Sequentially processes every transaction in df (chronological),\n",
    "# updating the BLR posterior after *each* observation.\n",
    "#\n",
    "# For each row i:\n",
    "#   1. Sample θᵢ ~ N(μᵢ, Σᵢ)      ← Thompson step\n",
    "#   2. Optimise (p−c)·vθᵢ(p) over a grid → p*\n",
    "#   3. Log p*  (recommendation)            (simulation only)\n",
    "#   4. Use the row's historical (price, qty) as the environment\n",
    "#      outcome (offline replay).\n",
    "#   5. Kalman‑update posterior with that (x, y).\n",
    "#   6. Record all diagnostics for later plots.\n",
    "#\n",
    "# Output (in‑memory):\n",
    "#   logs  – dict of numpy arrays  (len = N transactions)\n",
    "#   post_mean_final, post_cov_final – final posterior after last obs\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 0) Prepare numpy arrays for speed ---------------------------------------\n",
    "N = len(df)\n",
    "\n",
    "rec_price         = np.empty(N)                    # Thompson‑recommended\n",
    "act_price         = df['price'].values             # historical price\n",
    "act_qty           = df['quantity'].values          # observed demand\n",
    "pred_qty          = np.empty(N)                    # μᵢ prediction before update\n",
    "profit            = np.empty(N)\n",
    "cum_profit        = np.empty(N)\n",
    "beta_price_mean   = np.empty(N)                    # posterior mean of *linear* price term\n",
    "theta_mean_trace  = np.empty((N, len(feature_cols)))  # optional full trace\n",
    "\n",
    "# 1) Initial posterior (from Block 2) -------------------------------------\n",
    "mu  = post_mean.copy()\n",
    "Sig = post_cov.copy()\n",
    "\n",
    "# 2) Online loop ----------------------------------------------------------\n",
    "for i, row in tqdm(df.iterrows(), total=N, ncols=80,\n",
    "                   desc=\"Online BLR update\"):\n",
    "\n",
    "    # Static context vector (no price terms) for this transaction\n",
    "    static_vec = build_static_vector(row)\n",
    "\n",
    "    # 2.a Thompson‑sampling draw\n",
    "    theta_sample = sample_theta(mu, Sig)\n",
    "\n",
    "    # 2.b Recommend price p* (grid search)\n",
    "    p_star = find_optimal_price(theta_sample, static_vec)\n",
    "    rec_price[i] = p_star\n",
    "\n",
    "    # 2.c One‑step‑ahead demand prediction at *actual* historical price\n",
    "    x_pred = make_feature_vector(row, row['price'])\n",
    "    pred_qty[i] = max(mu @ x_pred, 0.0)\n",
    "\n",
    "    # 2.d Profit using historical price (offline replay)\n",
    "    #     If 'cost' column exists use per‑row cost, else global constant\n",
    "    cost_i  = row['cost'] if 'cost' in df.columns else COST_PER_UNIT\n",
    "    profit_i = (row['price'] - cost_i) * row['quantity']\n",
    "    profit[i] = profit_i\n",
    "    cum_profit[i] = profit_i if i == 0 else cum_profit[i-1] + profit_i\n",
    "\n",
    "    # 2.e Posterior update with (x_obs, y_obs)\n",
    "    x_obs = x_pred                               # same vector\n",
    "    y_obs = row['quantity']\n",
    "    mu, Sig = blr_update(mu, Sig, x_obs, y_obs)\n",
    "\n",
    "    # 2.f Logs for diagnostics\n",
    "    beta_price_mean[i]  = mu[feat_idx['price']]\n",
    "    theta_mean_trace[i] = mu                     # optional full trace\n",
    "\n",
    "# 3) Final posterior snapshot ---------------------------------------------\n",
    "post_mean_final = mu.copy()\n",
    "post_cov_final  = Sig.copy()\n",
    "\n",
    "print(f\"\\nFinal β_price mean   : {post_mean_final[feat_idx['price']]:.4f}\")\n",
    "print(f\"Total cumulative profit: {cum_profit[-1]:.2f}\")\n",
    "\n",
    "# 4) Pack logs dict --------------------------------------------------------\n",
    "logs = {\n",
    "    \"time\"                : df['date'].values,          # numpy datetime64\n",
    "    \"actual_price\"        : act_price,\n",
    "    \"recommended_price\"   : rec_price,\n",
    "    \"actual_quantity\"     : act_qty,\n",
    "    \"predicted_quantity\"  : pred_qty,\n",
    "    \"profit\"              : profit,\n",
    "    \"cum_profit\"          : cum_profit,\n",
    "    \"beta_price_mean\"     : beta_price_mean,\n",
    "    # heavy but nice to have:\n",
    "    \"theta_mean_trace\"    : theta_mean_trace,\n",
    "    # final posterior:\n",
    "    \"post_mean_final\"     : post_mean_final,\n",
    "    \"post_cov_final\"      : post_cov_final,\n",
    "    \"feature_cols\"        : feature_cols,\n",
    "    \"price_bounds\"        : (MIN_PRICE, MAX_PRICE),\n",
    "    \"sigma2\"              : sigma2,\n",
    "    \"cost_per_unit\"       : COST_PER_UNIT,\n",
    "}\n",
    "\n",
    "print(\"✔️  Block 4 complete – online learning finished and logs ready.\")\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 5 · Persist results to disk\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Saves everything created in Block 4 so you can reload later\n",
    "# without rerunning the online loop.\n",
    "#\n",
    "# Creates a folder:\n",
    "#       results/phase1_<YYYYMMDD_HHMMSS>/\n",
    "# Inside you’ll find:\n",
    "#   • transaction_logs.csv        – per‑tx price/qty/profit timeline\n",
    "#   • theta_mean_trace.npz        – (N, d) posterior mean at each step\n",
    "#   • beta_mean_final.npy         – final μ_T\n",
    "#   • beta_cov_final.npy          – final Σ_T\n",
    "#   • meta.json                   – small metadata file\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "import os, json, pickle, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Confirm logs exists -------------------------------------------------\n",
    "try:\n",
    "    logs\n",
    "except NameError:\n",
    "    raise RuntimeError(\"logs dict not found – run Block 4 first.\")\n",
    "\n",
    "# 2) Make results directory ------------------------------------------------\n",
    "timestamp   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_dir    = os.path.join(\"results\", f\"phase1_{timestamp}\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 3) Transaction‑level CSV -------------------------------------------------\n",
    "df_logs = pd.DataFrame({\n",
    "    \"time\"               : logs[\"time\"],\n",
    "    \"actual_price\"       : logs[\"actual_price\"],\n",
    "    \"recommended_price\"  : logs[\"recommended_price\"],\n",
    "    \"actual_quantity\"    : logs[\"actual_quantity\"],\n",
    "    \"predicted_quantity\" : logs[\"predicted_quantity\"],\n",
    "    \"profit\"             : logs[\"profit\"],\n",
    "    \"cum_profit\"         : logs[\"cum_profit\"],\n",
    "    \"beta_price_mean\"    : logs[\"beta_price_mean\"],\n",
    "})\n",
    "csv_path = os.path.join(base_dir, \"transaction_logs.csv\")\n",
    "df_logs.to_csv(csv_path, index_label=\"transaction_index\")\n",
    "print(f\"✓ Saved per‑transaction log → {csv_path}\")\n",
    "\n",
    "# 4) θ‑trace (posterior mean at each step) ---------------------------------\n",
    "theta_path = os.path.join(base_dir, \"theta_mean_trace.npz\")\n",
    "np.savez_compressed(theta_path, theta=logs[\"theta_mean_trace\"])\n",
    "print(f\"✓ Saved theta_mean_trace → {theta_path}\")\n",
    "\n",
    "# 5) Final posterior arrays -----------------------------------------------\n",
    "np.save(os.path.join(base_dir, \"beta_mean_final.npy\"), logs[\"post_mean_final\"])\n",
    "np.save(os.path.join(base_dir, \"beta_cov_final.npy\"),  logs[\"post_cov_final\"])\n",
    "print(\"✓ Saved final posterior μ & Σ (.npy files)\")\n",
    "\n",
    "# 6) Metadata --------------------------------------------------------------\n",
    "meta = {\n",
    "    \"timestamp\"      : timestamp,\n",
    "    \"feature_cols\"   : logs[\"feature_cols\"],\n",
    "    \"sigma2\"         : float(logs[\"sigma2\"]),\n",
    "    \"price_bounds\"   : [float(x) for x in logs[\"price_bounds\"]],\n",
    "    \"cost_per_unit\"  : float(logs[\"cost_per_unit\"]),\n",
    "    \"n_transactions\" : int(len(logs[\"actual_price\"])),\n",
    "}\n",
    "with open(os.path.join(base_dir, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"✓ Saved meta.json\")\n",
    "\n",
    "# 7) Helper loader for future notebooks -----------------------------------\n",
    "def load_phase1_results(folder_path):\n",
    "    \"\"\"\n",
    "    Quick re‑loader for everything saved by Block 5.\n",
    "    Returns (df_logs, theta_trace, mu_final, Sigma_final, meta)\n",
    "    \"\"\"\n",
    "    df_logs = pd.read_csv(os.path.join(folder_path, \"transaction_logs.csv\"),\n",
    "                          parse_dates=[\"time\"],\n",
    "                          index_col=\"transaction_index\")\n",
    "    theta   = np.load(os.path.join(folder_path, \"theta_mean_trace.npz\"))[\"theta\"]\n",
    "    mu_fin  = np.load(os.path.join(folder_path, \"beta_mean_final.npy\"))\n",
    "    Sig_fin = np.load(os.path.join(folder_path, \"beta_cov_final.npy\"))\n",
    "    with open(os.path.join(folder_path, \"meta.json\"), \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    return df_logs, theta, mu_fin, Sig_fin, meta\n",
    "\n",
    "print(\"\\n✔️  Block 5 complete – results persisted & loader defined.\")\n",
    "print(f\"   Folder: {base_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 6 · Visualisation suite\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Creates publication‑quality figures:\n",
    "#   6‑a Demand curve ±95 % CI vs. observed data\n",
    "#   6‑b Price‑quantity scatter + fitted curve\n",
    "#   6‑c Cumulative profit trajectory\n",
    "#   6‑d Posterior mean of β_price over time\n",
    "#   6‑e Histogram of one‑step residuals\n",
    "#\n",
    "# The block works in TWO modes:\n",
    "#   • In‑memory  – if logs dict (from Block 4) is present\n",
    "#   • Off‑disk   – provide results_folder pointing to a\n",
    "#                  results/phase1_YYYYMMDD_HHMMSS directory\n",
    "#\n",
    "# Figures are saved into <plots_dir> and also displayed.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Choose data source ----------------------------------------------------\n",
    "results_folder = None   # ← set to a saved folder OR leave None to use in‑RAM logs\n",
    "\n",
    "if results_folder is None:\n",
    "    try:\n",
    "        logs\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"No in‑memory logs found. \"\n",
    "                           \"Either run Blocks 4‑5 or set results_folder.\")\n",
    "    df_logs       = pd.DataFrame({\n",
    "        \"time\"              : logs[\"time\"],\n",
    "        \"actual_price\"      : logs[\"actual_price\"],\n",
    "        \"recommended_price\" : logs[\"recommended_price\"],\n",
    "        \"actual_quantity\"   : logs[\"actual_quantity\"],\n",
    "        \"predicted_quantity\": logs[\"predicted_quantity\"],\n",
    "        \"profit\"            : logs[\"profit\"],\n",
    "        \"cum_profit\"        : logs[\"cum_profit\"],\n",
    "        \"beta_price_mean\"   : logs[\"beta_price_mean\"],\n",
    "    })\n",
    "    theta_mean_trace = logs[\"theta_mean_trace\"]\n",
    "    mu_final         = logs[\"post_mean_final\"]\n",
    "    Sigma_final      = logs[\"post_cov_final\"]\n",
    "    price_bounds     = logs[\"price_bounds\"]\n",
    "    plots_dir        = os.path.join(\"results\", \"plots_inline\")\n",
    "else:\n",
    "    # Loader from Block 5\n",
    "    df_logs, theta_mean_trace, mu_final, Sigma_final, meta = load_phase1_results(results_folder)\n",
    "    price_bounds = meta[\"price_bounds\"]\n",
    "    plots_dir    = os.path.join(results_folder, \"plots\")\n",
    "\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "min_p, max_p = price_bounds\n",
    "N            = len(df_logs)\n",
    "\n",
    "# Helper: feature vector for baseline context (Sunday, avg annual)\n",
    "baseline_static = np.zeros(len(static_feature_cols) + 1)  # +1 for intercept\n",
    "baseline_static[0] = 1.0\n",
    "def demand_curve(mu, p_grid):\n",
    "    \"\"\"\n",
    "    Compute posterior mean demand for baseline context.\n",
    "    \"\"\"\n",
    "    # theta split: [static coeffs | price coeffs]\n",
    "    static_coeffs  = mu[:-len(price_basis_cols)]\n",
    "    price_coeffs   = mu[-len(price_basis_cols):]\n",
    "    demand = []\n",
    "    for p in p_grid:\n",
    "        basis = price_basis(p)\n",
    "        demand.append(static_coeffs @ baseline_static + price_coeffs @ basis)\n",
    "    return np.array(demand)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6‑a Demand curve with 95 % credible interval\n",
    "# ------------------------------------------------------------\n",
    "p_grid = np.linspace(min_p, max_p, 300)\n",
    "d_mean = demand_curve(mu_final, p_grid)\n",
    "# Credible interval (variance per point)\n",
    "var_vec = []\n",
    "for p in p_grid:\n",
    "    basis = price_basis(p)\n",
    "    phi   = np.concatenate([baseline_static, basis])\n",
    "    var_vec.append(phi @ Sigma_final @ phi)\n",
    "var_vec = np.array(var_vec)\n",
    "d_std   = np.sqrt(var_vec)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(p_grid, d_mean, label=\"Posterior mean demand\")\n",
    "plt.fill_between(p_grid, d_mean - 2*d_std, d_mean + 2*d_std,\n",
    "                 alpha=0.3, label=\"95% credible interval\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.title(\"Estimated demand curve (baseline context)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"demand_curve_ci.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6‑b Scatter of data + fitted curve\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.scatter(df_logs[\"actual_price\"], df_logs[\"actual_quantity\"],\n",
    "            s=10, alpha=0.25, label=\"Observed transactions\")\n",
    "plt.plot(p_grid, d_mean, linewidth=2, label=\"Posterior mean\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.title(\"Observed data vs. fitted demand curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"scatter_fit.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6‑c Cumulative profit\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_logs[\"cum_profit\"].values)\n",
    "plt.xlabel(\"Transaction index\")\n",
    "plt.ylabel(\"Cumulative profit\")\n",
    "plt.title(\"Cumulative profit over time (historical prices)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"cum_profit.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6‑d Posterior mean β_price evolution\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_logs[\"beta_price_mean\"].values)\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "plt.xlabel(\"Transaction index\")\n",
    "plt.ylabel(\"Posterior mean β_price\")\n",
    "plt.title(\"Convergence of linear price coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"beta_price_trace.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6‑e Residual histogram\n",
    "# ------------------------------------------------------------\n",
    "residuals = df_logs[\"actual_quantity\"].values - df_logs[\"predicted_quantity\"].values\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residuals, bins=50, edgecolor=\"k\", alpha=0.8)\n",
    "plt.xlabel(\"Residual (actual − predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of 1‑step residuals\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"residual_hist.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✔️  Block 6 finished – plots saved to {plots_dir}\")\n",
    "print(\"   Files:\")\n",
    "for f in os.listdir(plots_dir):\n",
    "    print(\"   •\", f)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Compute final‑posterior optimal price (baseline context)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "\n",
    "# 1)  Baseline context: intercept = 1, Sunday (all dummies 0), average season\n",
    "baseline_static = np.zeros(len(static_feature_cols) + 1)   # +1 for intercept\n",
    "baseline_static[0] = 1.0\n",
    "\n",
    "# Split posterior mean into static / price blocks\n",
    "n_static = len(baseline_static)\n",
    "static_coef = mu_final[:n_static]                 # β for intercept + seasonality\n",
    "price_coef  = mu_final[n_static:]                 # β for price bases\n",
    "\n",
    "# 2)  Vectorised demand‑mean function ------------------------\n",
    "def demand_mean_vec(p_array):\n",
    "    \"\"\"\n",
    "    Posterior‑mean demand for a NumPy array of prices, baseline context.\n",
    "    p_array: shape (m,)\n",
    "    Returns   shape (m,)\n",
    "    \"\"\"\n",
    "    p = np.asarray(p_array)\n",
    "    # Build price‑basis matrix   shape (m, 4)\n",
    "    eps = 1e-12                 # avoid log(0)\n",
    "    PB = np.stack([\n",
    "        p,\n",
    "        p ** 2,\n",
    "        np.log(p + eps),\n",
    "        np.tanh(p / TANH_SCALE)\n",
    "    ], axis=1)\n",
    "    const_part = baseline_static @ static_coef    # scalar\n",
    "    return const_part + PB @ price_coef           # vector\n",
    "\n",
    "# 3)  Profit grid search ------------------------------------\n",
    "grid = np.linspace(MIN_PRICE, MAX_PRICE, 2000)    # dense grid\n",
    "demand_hat = np.maximum(demand_mean_vec(grid), 0) # clip negative preds\n",
    "exp_profit = (grid - COST_PER_UNIT) * demand_hat\n",
    "\n",
    "idx_best   = int(np.argmax(exp_profit))\n",
    "opt_price  = grid[idx_best]\n",
    "opt_demand = demand_hat[idx_best]\n",
    "opt_profit = exp_profit[idx_best]\n",
    "\n",
    "# 4)  Display results ---------------------------------------\n",
    "print(f\"──────── Optimal price from final posterior (baseline context) ────────\")\n",
    "print(f\"Optimal price  : {opt_price:,.2f}\")\n",
    "print(f\"Exp. demand    : {opt_demand:,.2f} units/tx\")\n",
    "print(f\"Exp. profit/tx : {opt_profit:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Block 7 · Figure‑4‑style Thompson‑Sampling illustration\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Recreates the three‑panel conceptual figure from the PVD‑B\n",
    "# paper for *any* transaction index τ:\n",
    "#   1. Posterior mean demand ±95 % CI (black)\n",
    "#   2. One Thompson‑sample realisation v_TS(p)   (green)\n",
    "#   3. Profit curve (p−c)·v_TS(p)                (red)\n",
    "#   4. Vertical line at p*τ  (argmax of profit curve)\n",
    "#\n",
    "# Usage:\n",
    "#     plot_ts_realisation(idx=5000, use_final_posterior=False)\n",
    "#\n",
    "# If use_final_posterior=True, the demand envelope uses the\n",
    "# final posterior; otherwise it uses the posterior *after* the\n",
    "# specified tx (mu = theta_mean_trace[idx]).\n",
    "#\n",
    "# NOTE:  We did not store Σ_t at every step (memory).  We use\n",
    "#        the *final* covariance as a proxy for the envelope.\n",
    "#        If you saved Σ_t per iteration, replace the proxy.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ts_realisation(idx,\n",
    "                        price_bounds=(MIN_PRICE, MAX_PRICE),\n",
    "                        grid_size=400,\n",
    "                        use_final_posterior=False,\n",
    "                        context=\"row\"):        # \"row\" | \"baseline\"\n",
    "    \"\"\"\n",
    "    idx : 0‑based transaction index τ\n",
    "    context:\n",
    "        • \"row\"      → use the actual context of transaction τ\n",
    "        • \"baseline\" → Sunday + average season (same as Block 6 curve)\n",
    "    \"\"\"\n",
    "    # 1) Choose posterior parameters --------------------------\n",
    "    if use_final_posterior:\n",
    "        mu = mu_final\n",
    "        Sig= Sigma_final\n",
    "    else:\n",
    "        mu = theta_mean_trace[idx]\n",
    "        Sig= Sigma_final          # proxy for uncertainty envelope\n",
    "\n",
    "    # 2) Context static vector -------------------------------\n",
    "    if context == \"baseline\":\n",
    "        static_vec = baseline_static\n",
    "    else:\n",
    "        row = df.iloc[idx]\n",
    "        static_vec = build_static_vector(row)\n",
    "\n",
    "    # 3) Build price grid ------------------------------------\n",
    "    p_grid = np.linspace(price_bounds[0], price_bounds[1], grid_size)\n",
    "\n",
    "    # 3a Posterior mean demand\n",
    "    static_coef = mu[:len(static_vec)]\n",
    "    price_coef  = mu[len(static_vec):]\n",
    "    PB          = np.stack([\n",
    "        p_grid,\n",
    "        p_grid**2,\n",
    "        np.log(p_grid),\n",
    "        np.tanh(p_grid / TANH_SCALE)\n",
    "    ], axis=1)\n",
    "    d_mean = static_vec @ static_coef + PB @ price_coef\n",
    "\n",
    "    # 3b 95 % CI using proxy Σ\n",
    "    var_vec = []\n",
    "    for p in p_grid:\n",
    "        phi = np.concatenate([static_vec,\n",
    "                              [p, p**2, np.log(p), np.tanh(p / TANH_SCALE)]])\n",
    "        var_vec.append(phi @ Sig @ phi)\n",
    "    d_std = np.sqrt(np.maximum(var_vec, 0))\n",
    "    upper, lower = d_mean + 2*d_std, d_mean - 2*d_std\n",
    "\n",
    "    # 4) Thompson‑sampled curve ------------------------------\n",
    "    theta_ts = sample_theta(mu, Sig)\n",
    "    d_ts = static_vec @ theta_ts[:len(static_vec)] + PB @ theta_ts[len(static_vec):]\n",
    "\n",
    "    # 5) Profit function & argmax ----------------------------\n",
    "    profit_ts = (p_grid - COST_PER_UNIT) * np.maximum(d_ts, 0)\n",
    "    p_star = p_grid[np.argmax(profit_ts)]\n",
    "    max_profit = np.max(profit_ts)\n",
    "\n",
    "    # 6) Plot -------------------------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "    # Panel 1 – demand curves\n",
    "    ax1.plot(p_grid, d_mean, color=\"black\", label=\"Posterior mean\")\n",
    "    ax1.fill_between(p_grid, lower, upper,\n",
    "                     color=\"steelblue\", alpha=0.3, label=\"95% CI\")\n",
    "    ax1.plot(p_grid, d_ts, color=\"green\", label=\"TS realisation\")\n",
    "    ax1.set_xlabel(\"Price\"); ax1.set_ylabel(\"Quantity\")\n",
    "    ax1.set_title(f\"Demand curves at τ={idx}\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Panel 2 – profit curve + optimal price\n",
    "    ax2.plot(p_grid, profit_ts, color=\"red\", label=\"Profit (TS sample)\")\n",
    "    ax2.axvline(p_star, color=\"black\", linestyle=\"--\", label=f\"p*={p_star:.1f}\")\n",
    "    ax2.set_xlabel(\"Price\"); ax2.set_ylabel(\"Profit\")\n",
    "    ax2.set_title(\"Objective function\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"ts_realisation_tau{idx}.png\"\n",
    "    plt.savefig(os.path.join(plots_dir, fname), dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"✓ Figure saved to {os.path.join(plots_dir, fname)}\")\n",
    "\n",
    "# Example call\n",
    "# plot_ts_realisation(idx=5000, use_final_posterior=False, context=\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eece45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "#  SNAPSHOT MODEL‑2 RESULTS  →  results_model_2\n",
    "#  • Run immediately after Model‑2’s training & persistence cells.\n",
    "#  • No re‑execution of the model; just bundle what’s already there.\n",
    "#  • Automatic fallback: reloads the latest phase1_* folder on disk\n",
    "#    if the expected in‑memory objects are missing.\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os\n",
    "from glob import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Check whether Model‑2 artefacts are still in RAM\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "_have_mem = all(name in globals() for name in [\n",
    "    \"logs\",               # big dict created in Block 4\n",
    "    \"df_logs\",            # pretty transaction DataFrame (Block 5)\n",
    "    \"theta_mean_trace\",   # posterior path\n",
    "    \"post_mean_final\",    # final μ\n",
    "    \"post_cov_final\",     # final Σ\n",
    "])\n",
    "\n",
    "if _have_mem:\n",
    "    print(\"✓ Model‑2 artefacts found in memory – using them directly.\")\n",
    "    model2_folder = globals().get(\"base_dir\", \"\") or \"‑in‑memory‑run‑\"\n",
    "    df_logs2      = df_logs\n",
    "    theta2        = theta_mean_trace\n",
    "    mu2           = post_mean_final\n",
    "    Sigma2        = post_cov_final\n",
    "    meta2         = {\n",
    "        \"feature_cols\" : logs[\"feature_cols\"],\n",
    "        \"price_bounds\" : logs[\"price_bounds\"],\n",
    "        \"sigma2\"       : logs[\"sigma2\"],\n",
    "        \"cost_per_unit\": logs[\"cost_per_unit\"],\n",
    "        \"folder\"       : model2_folder\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # ────────────────────────────────────────────────────────────\n",
    "    # 2.  Fallback – reload the *newest* “phase1_*” folder\n",
    "    # ────────────────────────────────────────────────────────────\n",
    "    try:\n",
    "        load_phase1_results          # helper defined in Block 5\n",
    "    except NameError as err:\n",
    "        raise NameError(\"Model‑2 objects not in RAM and helper \"\n",
    "                        \"`load_phase1_results()` is missing. \"\n",
    "                        \"Run Model‑2’s persistence block first.\") from err\n",
    "\n",
    "    phase_dirs = sorted(\n",
    "        glob(os.path.join(\"results\", \"phase1_*\")),\n",
    "        key=os.path.getmtime\n",
    "    )\n",
    "    if not phase_dirs:\n",
    "        raise FileNotFoundError(\"No phase1_* folders found in ./results/. \"\n",
    "                                \"Run Model‑2 first.\")\n",
    "    model2_folder = phase_dirs[-1]   # latest folder\n",
    "    print(f\"ℹ Model‑2 artefacts reloaded from  {model2_folder}\")\n",
    "    df_logs2, theta2, mu2, Sigma2, meta2 = load_phase1_results(model2_folder)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3.  Bundle everything neatly into `results_model_2`\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "results_model_2 = {\n",
    "    \"folder\"     : model2_folder,\n",
    "    \"df_logs\"    : df_logs2,      # per‑transaction DataFrame\n",
    "    \"theta_trace\": theta2,        # (N, d) posterior mean over time\n",
    "    \"mu_final\"   : mu2,           # final posterior mean\n",
    "    \"Sigma_final\": Sigma2,        # final posterior covariance\n",
    "    \"meta\"       : meta2          # misc. meta‑info\n",
    "}\n",
    "\n",
    "print(\"\\nresults_model_2 is now available with keys:\")\n",
    "pprint.pp(results_model_2.keys())\n",
    "print(f\"\\n↳ Total profit recorded by Model‑2 = \"\n",
    "      f\"{results_model_2['df_logs']['profit'].sum():,.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
